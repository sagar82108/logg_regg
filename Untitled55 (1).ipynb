{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e775db-5e70-46a2-bb0b-f7ceee691e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear Regression: Used for predicting continuous numerical values. \n",
    "                   Provides a linear relationship between variables.\n",
    "\n",
    "Logistic Regression: Used for binary or categorical outcomes, predicting probabilities between 0 and 1. \n",
    "                     Ideal for binary classification scenarios, such as yes/no or pass/fail predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9ece7-a0b1-4132-8e4c-8f339abea673",
   "metadata": {},
   "outputs": [],
   "source": [
    "In logistic regression, the cost function used is the logistic loss (log loss) function. It measures the error between\n",
    "predicted probabilities and actual binary outcomes.\n",
    "Optimization is typically done using gradient descent,\n",
    "adjusting model parameters iteratively to minimize the cost and improve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a416d-5fa9-48d5-ae5c-14103182cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization in logistic regression prevents overfitting by adding a penalty to the model's complexity.\n",
    "It discourages large coefficient values, leading to simpler and more generalizable models.\n",
    "L1 (Lasso) regularization can even eliminate irrelevant features, further reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65325f2-bb22-4b31-bb4c-88346d756012",
   "metadata": {},
   "outputs": [],
   "source": [
    "The ROC curve is a visual tool to evaluate the performance of binary classification models like logistic regression. \n",
    "It shows how well the model distinguishes between true positives and false positives at various classification thresholds. The Area Under the Curve (AUC) quantifies this performance,\n",
    "with a higher AUC indicating better model discrimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dae817-32b8-4e05-89c9-a1586e23034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Common techniques for feature selection in logistic regression include statistical tests, regularization (L1), tree-based feature importance, and domain knowledge.\n",
    "These techniques help improve the model's performance by reducing overfitting, enhancing interpretability, increasing efficiency, and enhancing robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960b1f5-546a-4902-a468-b0260935aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "To handle imbalanced datasets in logistic regression:\n",
    "\n",
    "1. **Resampling**: Over-sample the minority class or under-sample the majority class to balance the dataset.\n",
    "\n",
    "2. **Weighted Loss**: Modify the loss function to penalize errors in the minority class more.\n",
    "\n",
    "3. **Ensemble Methods**: Use bagging or boosting techniques to combine multiple models.\n",
    "\n",
    "4. **Anomaly Detection**: Treat the minority class as anomalies and use relevant algorithms.\n",
    "\n",
    "5. **Data-level Techniques**: Collect more data for the minority class or augment existing data.\n",
    "\n",
    "6. **Threshold Adjustment**: Adjust classification thresholds to optimize for precision or recall.\n",
    "\n",
    "7. **Cost-sensitive Learning**: Assign different misclassification costs for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcc06d-af23-411f-a5ae-c8fcc8c31f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Common issues in logistic regression and their solutions:\n",
    "\n",
    "1. **Multicollinearity**:\n",
    "   - Address by identifying and removing correlated variables or using dimensionality reduction techniques like PCA.\n",
    "\n",
    "2. **Overfitting**:\n",
    "   - Prevent overfitting with regularization (L1 or L2) and cross-validation.\n",
    "\n",
    "3. **Imbalanced Data**:\n",
    "   - Handle class imbalance with resampling, weighted loss functions, or ensemble methods.\n",
    "\n",
    "4. **Non-linearity**:\n",
    "   - Capture non-linear relationships by transforming variables or using polynomial features.\n",
    "\n",
    "5. **Model Interpretability**:\n",
    "   - While logistic regression is interpretable, complex models may require simplified representations or feature engineering for better understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
